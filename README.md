# LetsUpgrade-AI-ML
Assignments submission under the LetsUpgrade AI- ML program <br/>
SYLLABUS<br/>
Lesson 1: Introduction to AI-ML-DL<br/>
Lesson 2: Python Introduction<br/>
Lesson 3: Python Data Structures<br/>
Lesson 4: Python Function<br/>
Lesson 5: Statistical Learning<br/>
Lesson 6: Numpy<br/>
Lesson 7: Pandas<br/>
Lesson 8: Supervised Learning Algorithm<br/>
Lesson 9: Performance Metrics<br/>
Lesson 10: Optimization principles<br/>
Lesson 11: Supervised Classification<br/>
Lesson 12: Un-Supervised Algorithms<br/>
Lesson 13: Ensemble Techniques<br/>
Lesson 14: Deep Learning Algorithm<br/>
Lesson 15: Deep Learning Framework<br/>
Lesson 16: OpenCV-image handling<br/>
Lesson 17: Image preprocessing<br/>
Lesson 18: Convolution Neural Networks<br/>
Lesson 19: Recurrent Neural Networks<br/>
Lesson 20: Natural Language Processing<br/>

IN DEPTH TOPICS COVERED\
Lesson 1: Introduction to AI-ML-DL<br/>
-ml definiton <br/>
-supervised , unsupervised learning and other types<br/>
Lesson 2: Python Introduction<br/>
Lesson 3: Python Data Structures<br/>
Lesson 4: Python Function<br/>
Lesson 5: Statistical Learning<br/>
-discriptive statistics<br/>
 --measure of central tendency(mean , mode , median)\
 --measure of dispersion..........\
 --distribution of shape(skewness, kurtosis, coefficient of variation)\
 --to find outliers( boxplot, scatterplot)\
-inferential statistics( 2 types)\
 --parametric test( for normal distribution)\
   =one sample t test, 2 sample paired test, 2 sample seperate t test, one sample f test, 2 sample f test\
 --non parametric test( no normal distribution, analysizing data without population)\
   =wilcoxin sign , feuedman test, man whitney test , kruskal wallies test, chi square test\
-inferntial statisitcs\
 --critical value approach ( hypothesis 7 steps)\
 --p value approach \
 --error in hypothesis https://dfrieds.com/math/errors-hypothesis-testing.html\
-correlation( coefficient of determination, correaltion coefficient , correaltion classification)\
-practicals on inferential statistics parametric and non parametric \
-under parametirc , you have two more test:\
 --annova( diff types of anova like 2 way annova , how to perform one way anova)\
 --ancova \
 --connection between anova , ancova , regression\
Lesson 6: Numpy<br/>
-using numpy for structured list if numbers, images matrices, array \
-discuss array in numpy , various numpy functions\
-slicing \
-boolean array indexing \
-array math\
-combining array in numpy \
-array reshaping \
Lesson 7: Pandas<br/>
-key features of pandas\
-panda data structure: series, dataframe and panel\
-practicals on how to go about with missing values\
-exploratory data analysis \
 --using boxplot , correlation, heatmap (also pairplot, barplot, scatterplot etc)\
Lesson 8: Supervised Learning Algorithm<br/>
- https://miro.medium.com/max/1436/1*k2bLmeYIG7z7dCyxADedhQ.png linear regression\
 --regression vs classification\
 --linear regression performance metrices, error estimation and cost function , implementation steps\
 --types of statistical variables : dependent and independent (for regression both should be continuous)\
 --assumtions of regression( heteroskedasticity, multi colinearity, auto correlation)\
-multiple linear regression https://slideplayer.com/slide/3264326/11/images/4/Multiple+Regression+Model.jpg\
-logistic regression http://faculty.cas.usf.edu/mbrannick/regression/gifs/lo4.gif\
 --IV categorical, continuous, DV binary categorical\
 --p val<0.05 where 0 is for the most important feature\
 --understand how to use the summary coefficient\
 --diff between linear and logistic regression\
-decision tree\
 --diff bw correlation and regression\
 --IV categorical and continuous, DV categorical\
 -- decision tree does both classification and prediction\
 --variable type and split type\
 --node impurity\
 --decision tree algos( ID3, CART)\
 --adv disadv of decision trees\
 --apply decision tree and get accuracy and webgraphviz\
-random forest (ensemble of decision tree)\
 --finding important features through random forest\
 --using decision tree over these features and finding accuracy\
-Naive bayes\
 --IV, DV are categorical\
 --naive bayes theorem (conditional probablity)\
 --how to read a confusion matrix\
-KNN\
 --classification using euclidians distance\
 --choosing k value, finding the best k value\
 --practicals : choose random k values, keep changing k to find the best k\
-SVM.........................................................................\
 --drawing hyperplanes\
 --point close to hyperplane is called SV\
 --margin linear is the distance beteen 2 Sv\
Lesson 12: Un-Supervised Algorithms<br/>
-association rule ( for building recommendation systems to find frequently occuring combination of items)\
-support : prob of buying of an item\
-confidence: prob of buying a combination of items\
-methods: MBA( fast and slow moving itemset grouped), Aripori algo(frequently purchased itemset)\
-association mining \
-market basket analysis\
-k means clustering\
 --classify data without first having been trained with labeled data\
 --apllication: CV, search engine\
 --steps for the algo\
 --find the elbow which will give the right value of k\
 --day 30 LDA, PCA..............................................................\

 

  
